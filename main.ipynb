{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import efficientnet_builder\n",
    "import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "  path = './'\n",
    "  train = pd.read_csv(path+'CheXpert-v1.0-small/train.csv')\n",
    "  valid = pd.read_csv(path+'CheXpert-v1.0-small/valid.csv')\n",
    "  \n",
    "  train['validation'] = False\n",
    "  valid['validation'] = True\n",
    "  df = pd.concat([train, valid])\n",
    "  \n",
    "  columns = ['Path', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'validation']\n",
    "  df = df[columns]\n",
    "  \n",
    "  for feature in ['Atelectasis', 'Edema']:\n",
    "      df[feature] = df[feature].apply(lambda x: 1 if x==-1 else x)\n",
    "  \n",
    "  for feature in ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']:\n",
    "      df[feature] = df[feature].apply(lambda x: 0 if x==-1 else x)\n",
    "  df.fillna(0, inplace=True)\n",
    "  \n",
    "  train = df[~df.validation][:50]\n",
    "  print(len(train))\n",
    "  train_files = train['Path'].tolist()\n",
    "  train_files = [path+fil for fil in train_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  train_labels = np.array(train[columns])\n",
    "  \n",
    "  valid = df[df.validation][:50]\n",
    "  print(len(valid))\n",
    "  valid_files = valid['Path'].tolist()\n",
    "  valid_files = [path+fil for fil in valid_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  valid_labels = np.array(valid[columns])  \n",
    "  return train_files, train_labels, valid_files, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size 224\n"
     ]
    }
   ],
   "source": [
    "MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "\n",
    "model_name='efficientnet-b0'\n",
    "batch_size=32\n",
    "\"\"\"Initialize internal variables.\"\"\"\n",
    "model_name = model_name\n",
    "batch_size = batch_size\n",
    "num_classes = 1000\n",
    "# Model Scaling parameters\n",
    "_, _, image_size, _ = efficientnet_builder.efficientnet_params(\n",
    "      model_name)\n",
    "print('image_size', image_size)\n",
    "\n",
    "def restore_model(sess, ckpt_dir):\n",
    "  \"\"\"Restore variables from checkpoint dir.\"\"\"\n",
    "  checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
    "  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "  ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\n",
    "  for v in tf.global_variables():\n",
    "    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n",
    "      ema_vars.append(v)\n",
    "  ema_vars = list(set(ema_vars))\n",
    "  var_dict = ema.variables_to_restore(ema_vars)\n",
    "  saver = tf.train.Saver(var_dict, max_to_keep=1)\n",
    "  saver.restore(sess, checkpoint)\n",
    "  return saver\n",
    "\n",
    "def build_model(features, is_training):\n",
    "  \"\"\"Build model with input features.\"\"\"\n",
    "  features -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  features /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  out, _ = efficientnet_builder.build_model_base(\n",
    "      features, model_name, is_training)\n",
    "  return out\n",
    "\n",
    "def build_dataset(filenames, labels, is_training):\n",
    "  \"\"\"Build input dataset.\"\"\"\n",
    "  filenames = tf.constant(filenames)\n",
    "  labels = tf.constant(labels)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "  def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = preprocessing.preprocess_image(\n",
    "        image_string, is_training, image_size=image_size)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "  dataset = dataset.map(_parse_function)\n",
    "  dataset = dataset.batch(batch_size)#.repeat()\n",
    "  return dataset\n",
    "\n",
    "def _loss(x, y):\n",
    "  logits = tf.contrib.layers.fully_connected(x, 5, activation_fn=None)  \n",
    "  predicts = tf.math.sigmoid(logits, name = 'sigmoid_logits')\n",
    "  cross_entropy = tf.losses.sigmoid_cross_entropy(logits=logits,\n",
    "                                                  multi_class_labels=y)\n",
    "  weight_decay = 1e-5\n",
    "  loss = cross_entropy + weight_decay * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "       if 'batch_normalization' not in v.name])\n",
    "  return loss, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "1 1\n",
      "prediction (32, 5)\n",
      "prediction (18, 5)\n",
      "epoch: 1 | training loss: 1.01157402992 | validation loss: 3.23462748528\n",
      "prediction (32, 5)\n",
      "prediction (18, 5)\n",
      "epoch: 2 | training loss: 1.02353274822 | validation loss: 3.23245167732\n",
      "prediction (32, 5)\n",
      "prediction (18, 5)\n",
      "epoch: 3 | training loss: 0.930390894413 | validation loss: 3.22538077831\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  train_files, train_labels, validation_files, validation_labels = get_data()\n",
    "  train_dataset = build_dataset(train_files, train_labels, True)\n",
    "  valid_dataset = build_dataset(validation_files, validation_labels, False)\n",
    "  \n",
    "  train_iter = train_dataset.make_initializable_iterator()\n",
    "  valid_iter = valid_dataset.make_initializable_iterator()\n",
    "  \n",
    "  train_images, train_labels = train_iter.get_next()\n",
    "  valid_images, valid_labels = valid_iter.get_next()\n",
    "\n",
    "  train_out = build_model(train_images, is_training=True)\n",
    "  train_out = tf.reduce_mean(train_out, axis=[1,2])\n",
    "\n",
    "\n",
    "  with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    valid_out = build_model(valid_images, is_training=False)\n",
    "    valid_out = tf.reduce_mean(valid_out, axis=[1,2])\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  saver = restore_model(sess, './weights_efficientnet-b0/')\n",
    "\n",
    "  temp = set(tf.all_variables())\n",
    "\n",
    "  train_loss, _ = _loss(train_out, train_labels)\n",
    "  valid_loss, predicts = _loss(valid_out, valid_labels)\n",
    "  \n",
    "  lr = 0.0001\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=lr, name=\"adam\").minimize(train_loss)\n",
    "  \n",
    "  sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "  epochs = 3\n",
    "  n_batches_train = len(train_files)//batch_size\n",
    "  n_batches_valid = len(valid_files)//batch_size\n",
    "  saver = tf.train.Saver()\n",
    "  print(n_batches_train, n_batches_valid)\n",
    "  for epoch in range(epochs):\n",
    "    # training\n",
    "    sess.run(train_iter.initializer)\n",
    "    _train_loss = 0\n",
    "    for i in range(n_batches_train):\n",
    "      loss_value, _ = sess.run([train_loss, optimizer])\n",
    "      if i>1 and i%1000==0:\n",
    "        print('iteration: {} | loss {}'.format(i, loss_value))\n",
    "      _train_loss += loss_value\n",
    "    _train_loss = _train_loss/n_batches_train\n",
    "    \n",
    "    # validation\n",
    "    sess.run(valid_iter.initializer)\n",
    "    _valid_loss = 0\n",
    "    predictions = []\n",
    "    for i in range(n_batches_valid+1):\n",
    "      loss_value, prediction = sess.run([valid_loss, predicts])\n",
    "      _valid_loss += loss_value\n",
    "      predictions.append(prediction)\n",
    "      print('prediction', prediction.shape)\n",
    "    _valid_loss = _valid_loss/n_batches_valid\n",
    "#     calculate_metric(predictions, validation_labels)\n",
    "    \n",
    "    print(\"epoch: {} | training loss: {} | validation loss: {}\".format(epoch+1, _train_loss, _valid_loss ))\n",
    "    saver.save(sess, \"./saved_model/model{}\".format(model_name))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_metric(predictions, valid_labels):\n",
    "  print(predictions, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  restore_model(sess, './weights_efficientnet-b0/')\n",
    "  \n",
    "  sess.run(train_iter.initializer)\n",
    "  sess.run(valid_iter.initializer)\n",
    "  \n",
    "  n_batches = len(train_files)//batch_size\n",
    "  print(n_batches, batch_size)\n",
    "  for i in range(n_batches):\n",
    "    out1 = sess.run(train_out)\n",
    "    out2 = sess.run(valid_out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = tf.constant(np.ones((100, 2)), 'float32')\n",
    "X_val = tf.constant(np.zeros((10, 2)), 'float32')\n",
    "\n",
    "iter_train = tf.data.Dataset.from_tensor_slices(\n",
    "    X_train).make_initializable_iterator()\n",
    "iter_val = tf.data.Dataset.from_tensor_slices(\n",
    "    X_val).make_initializable_iterator()\n",
    "\n",
    "\n",
    "def graph(x, is_train=True):\n",
    "  return x\n",
    "\n",
    "\n",
    "output_train = graph(iter_train.get_next(), is_train=True)\n",
    "output_val = graph(iter_val.get_next(), is_train=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(iter_train.initializer)\n",
    "  sess.run(iter_val.initializer)\n",
    "\n",
    "  for train_iter in range(100):\n",
    "    print(sess.run(output_train))\n",
    "\n",
    "  for train_iter in range(10):\n",
    "    print(sess.run(output_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feature extract\n",
    "train_files, train_labels, valid_files, valid_labels = get_data()\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  train_images, train_labels = build_dataset(train_files, train_labels, False)\n",
    "  train_out = build_model(train_images, is_training=False)\n",
    "  train_out = tf.reduce_mean(train_out, axis=[1,2])\n",
    "\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  restore_model(sess, './weights_efficientnet-b5/')\n",
    "  \n",
    "  n_batches = len(train_files)//batch_size\n",
    "  print(n_batches, batch_size)\n",
    "  outs=[]\n",
    "  for i in range(n_batches):\n",
    "    outs.append(sess.run(train_out))\n",
    "    if i%20==0:\n",
    "      print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(outs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./features_b5_file.npy', np.array(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_files, train_labels, valid_files, valid_labels = get_data()\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  train_images, train_labels = build_dataset(train_files, train_labels, True)\n",
    "  train_out = build_model(train_images, is_training=True)\n",
    "  train_out = tf.reduce_mean(train_out, axis=[1,2])\n",
    "\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  restore_model(sess, './weights_efficientnet-b0/')\n",
    "  \n",
    "  \n",
    "  temp = set(tf.all_variables())\n",
    "  \n",
    "  with tf.variable_scope(\"transfer\"):\n",
    "    fc = tf.contrib.layers.fully_connected(train_out, 1024, activation_fn=None)\n",
    "    fc = tf.contrib.layers.fully_connected(fc, 512, activation_fn=None)\n",
    "    logits = tf.contrib.layers.fully_connected(fc, 5, activation_fn=None)\n",
    "       \n",
    "    logits = tf.math.sigmoid(logits, name = 'sigmoid_logits')\n",
    "    cross_entropy = tf.losses.sigmoid_cross_entropy(logits=logits,\n",
    "                                                    multi_class_labels=train_labels)\n",
    "    weight_decay = 1e-5\n",
    "    loss = cross_entropy + weight_decay * tf.add_n(\n",
    "        [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "         if 'batch_normalization' not in v.name])\n",
    "    \n",
    "    lr = 0.01\n",
    "    var = tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"transfer/\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr, name=\"adam\").minimize(loss, var_list=var)\n",
    "  \n",
    "  sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "  epochs = 3\n",
    "  for epoch in range(epochs):\n",
    "    tot_loss = 0\n",
    "    n_batches = len(train_files)//batch_size\n",
    "    for i in range(n_batches):\n",
    "      loss_value, _ = sess.run([loss, optimizer])\n",
    "      if i%100==0:\n",
    "        print('epoch {} | iteration: {} | loss {}'.format(epoch, i, loss_value))\n",
    "      tot_loss += loss_value\n",
    "    print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in graph.get_operations():\n",
    "     print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph.get_tensor_by_name('truediv:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(out_probs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "  path = './../main/'\n",
    "  train = pd.read_csv(path+'CheXpert-v1.0-small/train.csv')\n",
    "  valid = pd.read_csv(path+'CheXpert-v1.0-small/valid.csv')\n",
    "  \n",
    "  train['validation'] = False\n",
    "  valid['validation'] = True\n",
    "  df = pd.concat([train, valid])\n",
    "  \n",
    "  columns = ['Path', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'validation']\n",
    "  df = df[columns]\n",
    "  \n",
    "  for feature in ['Atelectasis', 'Edema']:\n",
    "      df[feature] = df[feature].apply(lambda x: 1 if x==-1 else x)\n",
    "  \n",
    "  for feature in ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']:\n",
    "      df[feature] = df[feature].apply(lambda x: 0 if x==-1 else x)\n",
    "  df.fillna(0, inplace=True)\n",
    "  \n",
    "  train = df[~df.validation][:300]\n",
    "  print(len(train))\n",
    "  train_files = train['Path'].tolist()\n",
    "  train_files = [path+fil for fil in train_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  train_labels = np.array(train[columns])\n",
    "  \n",
    "  valid = df[df.validation]\n",
    "  print(len(valid))\n",
    "  valid_files = valid['Path'].tolist()\n",
    "  valid_files = [path+fil for fil in valid_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  valid_labels = np.array(valid[columns])  \n",
    "  return train_files, train_labels, valid_files, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files, train_labels, valid_files, valid_labels = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def plot_images(dataset, n_images, samples_per_image):\n",
    "#     output = np.zeros((32 * n_images, 32 * samples_per_image, 3))\n",
    "\n",
    "#     row = 0\n",
    "#     for images, labls in dataset.repeat(samples_per_image).batch(n_images):\n",
    "#         print(images.shape, labls)\n",
    "#         break\n",
    "# #         output[:, row*32:(row+1)*32] = np.vstack(images.numpy())\n",
    "# #         row += 1\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(images[0])\n",
    "#     plt.show()\n",
    "# training_files, labels = get_data()    \n",
    "# training_files = tf.constant(training_files)\n",
    "# labels = tf.constant(labels, shape=[234, 5])\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((training_files, labels))\n",
    "# def _parse_function(filename, label):\n",
    "#   image_string = tf.read_file(filename)\n",
    "#   image_decoded = preprocessing.preprocess_image(\n",
    "#       image_string, False, image_size=image_size)\n",
    "#   image = tf.cast(image_decoded, tf.float32)\n",
    "#   return image, label\n",
    "\n",
    "# dataset = dataset.map(_parse_function)\n",
    "# plot_images(dataset, n_images=4, samples_per_image=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficientnet",
   "language": "python",
   "name": "efficientnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
