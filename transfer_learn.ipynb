{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import efficientnet_builder\n",
    "import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "  path = './../main/'\n",
    "  train = pd.read_csv(path+'CheXpert-v1.0-small/train.csv')\n",
    "  valid = pd.read_csv(path+'CheXpert-v1.0-small/valid.csv')\n",
    "  \n",
    "  train['validation'] = False\n",
    "  valid['validation'] = True\n",
    "  df = pd.concat([train, valid])\n",
    "  \n",
    "  columns = ['Path', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'validation']\n",
    "  df = df[columns]\n",
    "  \n",
    "  for feature in ['Atelectasis', 'Edema']:\n",
    "      df[feature] = df[feature].apply(lambda x: 1 if x==-1 else x)\n",
    "  \n",
    "  for feature in ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']:\n",
    "      df[feature] = df[feature].apply(lambda x: 0 if x==-1 else x)\n",
    "  df.fillna(0, inplace=True)\n",
    "  \n",
    "  train = df[~df.validation][:500]\n",
    "  print(len(train))\n",
    "  train_files = train['Path'].tolist()\n",
    "  train_files = [path+fil for fil in train_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  train_labels = np.array(train[columns])\n",
    "  \n",
    "  valid = df[df.validation]#[:50]\n",
    "  print(len(valid))\n",
    "  valid_files = valid['Path'].tolist()\n",
    "  valid_files = [path+fil for fil in valid_files]\n",
    "  \n",
    "  columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  valid_labels = np.array(valid[columns])  \n",
    "  return train_files, train_labels, valid_files, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size 224\n"
     ]
    }
   ],
   "source": [
    "MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "\n",
    "model_name='efficientnet-b4'\n",
    "batch_size=32\n",
    "\"\"\"Initialize internal variables.\"\"\"\n",
    "model_name = model_name\n",
    "batch_size = batch_size\n",
    "num_classes = 1000\n",
    "# Model Scaling parameters\n",
    "_, _, image_size, _ = efficientnet_builder.efficientnet_params(\n",
    "      model_name)\n",
    "print('image_size', image_size)\n",
    "\n",
    "def restore_model(sess, ckpt_dir):\n",
    "  \"\"\"Restore variables from checkpoint dir.\"\"\"\n",
    "  checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
    "  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "  ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\n",
    "  for v in tf.global_variables():\n",
    "    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n",
    "      ema_vars.append(v)\n",
    "  ema_vars = list(set(ema_vars))\n",
    "  var_dict = ema.variables_to_restore(ema_vars)\n",
    "  saver = tf.train.Saver(var_dict, max_to_keep=1)\n",
    "  saver.restore(sess, checkpoint)\n",
    "  return saver\n",
    "\n",
    "def build_model(features, is_training):\n",
    "  \"\"\"Build model with input features.\"\"\"\n",
    "  features -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  features /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
    "  out, _ = efficientnet_builder.build_model_base(\n",
    "      features, model_name, is_training)\n",
    "  return out\n",
    "\n",
    "def build_dataset(filenames, labels, is_training):\n",
    "  \"\"\"Build input dataset.\"\"\"\n",
    "  filenames = tf.constant(filenames)\n",
    "  labels = tf.constant(labels)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "  def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = preprocessing.preprocess_image(\n",
    "        image_string, is_training, image_size=image_size)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "  dataset = dataset.map(_parse_function)\n",
    "  dataset = dataset.batch(batch_size)#.repeat()\n",
    "  return dataset\n",
    "\n",
    "\n",
    "\n",
    "def _loss(x, y):\n",
    "  logits = tf.contrib.layers.fully_connected(x, 5, activation_fn=None)  \n",
    "  predicts = tf.math.sigmoid(logits, name = 'sigmoid_logits')\n",
    "  cross_entropy = tf.losses.sigmoid_cross_entropy(logits=logits,\n",
    "                                                  multi_class_labels=y)\n",
    "  weight_decay = 1e-5\n",
    "  loss = cross_entropy + weight_decay * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "       if 'batch_normalization' not in v.name])\n",
    "  return loss, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(predictions, validation_labels):\n",
    "  labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "  auc_scores = {labels[i]: roc_auc_score(validation_labels[:predictions.shape[0],i],predictions[:,i]) for i in range(5)}\n",
    "  chexpert_auc_scores = {\n",
    "                         'Atelectasis':      0.858,\n",
    "                         'Cardiomegaly':     0.854,\n",
    "                         'Consolidation':    0.939,\n",
    "                         'Edema':            0.941,\n",
    "                         'Pleural Effusion': 0.936\n",
    "                        }\n",
    "  max_feat_len = max(map(len, labels))\n",
    "#   if epoch+1 == epochs:\n",
    "#     [print(f'{k: <{max_feat_len}}\\t auc: {auc_scores[k]:.3}\\t chexpert auc: {chexpert_auc_scores[k]:.3}\\t difference: {(chexpert_auc_scores[k]-auc_scores[k]):.3}') for k in labels]\n",
    "  \n",
    "  avg_chexpert_auc = sum(list(chexpert_auc_scores.values()))/len(chexpert_auc_scores.values())\n",
    "  avg_auc          = sum(list(auc_scores.values()))/len(auc_scores.values())\n",
    "  \n",
    "  print('Average auc: {} \\t CheXpert average auc {}'.format(avg_auc, avg_chexpert_auc))\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(lr = 0.001, epochs=3, ckpt_dir='./weights_{}/'.format(model_name)):\n",
    "  graph = tf.Graph()\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    train_files, train_labels, validation_files, validation_labels = get_data()\n",
    "    train_dataset = build_dataset(train_files, train_labels, True)\n",
    "    valid_dataset = build_dataset(validation_files, validation_labels, False)\n",
    "    \n",
    "    train_iter = train_dataset.make_initializable_iterator()\n",
    "    valid_iter = valid_dataset.make_initializable_iterator()\n",
    "    \n",
    "    train_images, train_labels = train_iter.get_next()\n",
    "    valid_images, valid_labels = valid_iter.get_next()\n",
    "  \n",
    "    train_out = build_model(train_images, is_training=True)\n",
    "    train_out = tf.reduce_mean(train_out, axis=[1,2])\n",
    "  \n",
    "  \n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "      valid_out = build_model(valid_images, is_training=False)\n",
    "      valid_out = tf.reduce_mean(valid_out, axis=[1,2])\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = restore_model(sess, ckpt_dir)\n",
    "  \n",
    "    temp = set(tf.all_variables())\n",
    "  \n",
    "    train_loss, _ = _loss(train_out, train_labels)\n",
    "    valid_loss, predicts = _loss(valid_out, valid_labels)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = lr\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate,\n",
    "                                                         global_step,\n",
    "                                                         100000, 0.96, \n",
    "                                                         staircase=True)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate, momentum=0.9, name=\"momentum_opt\").minimize(train_loss, global_step=global_step)\n",
    "      \n",
    "    sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "#     saver.restore(sess, './saved_model/model_efficientnet-b4_0.761')\n",
    "    \n",
    "    n_batches_train = len(train_files)//batch_size\n",
    "    n_batches_valid = len(validation_files)//batch_size\n",
    "    \n",
    "    print(n_batches_train, n_batches_valid)\n",
    "    print('training starts')\n",
    "    for epoch in range(epochs):\n",
    "      tic = time.time()\n",
    "      # training\n",
    "      sess.run(train_iter.initializer)\n",
    "      _train_loss = 0\n",
    "      for i in range(n_batches_train):\n",
    "        loss_value, _ = sess.run([train_loss, optimizer])\n",
    "        if i>1 and i%100==0:\n",
    "          print('iteration: {} | loss {}'.format(i, loss_value))\n",
    "        _train_loss += loss_value\n",
    "      _train_loss = _train_loss/n_batches_train\n",
    "      \n",
    "      # validation\n",
    "      sess.run(valid_iter.initializer)\n",
    "      _valid_loss = 0\n",
    "      predictions = []\n",
    "      for i in range(n_batches_valid+1):\n",
    "        loss_value, prediction = sess.run([valid_loss, predicts])\n",
    "        _valid_loss += loss_value\n",
    "        predictions.append(prediction)\n",
    "      _valid_loss = _valid_loss/n_batches_valid\n",
    "      calculate_metric(np.vstack(predictions), validation_labels)\n",
    "      print('elapsed time: {}'.format(time.time()-tic))\n",
    "      print(\"epoch: {} | training loss: {} | validation loss: {}\".format(epoch+1, _train_loss, _valid_loss ))\n",
    "      saver.save(sess, \"./saved_model/model_{}_{}\".format(model_name, np.round(_valid_loss, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "train(lr = 0.001, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1157/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
